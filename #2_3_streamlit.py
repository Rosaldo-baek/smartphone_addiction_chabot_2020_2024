

# =========================================================
# Streamlit ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ RAG ì±—ë´‡ (ì„±ëŠ¥ê°œì„ : ê·¸ë˜í”„ ì„¸ì…˜ 1íšŒ ìƒì„±)
# - í•µì‹¬ ë¡œì§(ë¼ìš°í„°/í”Œë˜ë„ˆ/ê²€ìƒ‰/ë‹µë³€/ê²€ì¦) êµ¬ì¡°ëŠ” ì›ë³¸ ìœ ì§€í•¨
# - Streamlitì—ì„œ ëŠë ¤ì§€ëŠ” ì£¼ì›ì¸(ë§¤ ì§ˆë¬¸ë§ˆë‹¤ graph.compile)ì„ ì œê±°í•¨
# - status_placeholderëŠ” ì„¸ì…˜ ìƒíƒœë¡œ ì£¼ì…í•´ì„œ ë…¸ë“œ í•¨ìˆ˜ê°€ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ í•¨
# =========================================================

import streamlit as st
import json
import re
import os
import pandas as pd
import tempfile
import uuid
from pathlib import Path
from typing import Dict, Any, List, Optional, TypedDict

# LangChain / LangGraph
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver


# =========================================================
# 0) Streamlit í˜ì´ì§€ ì„¤ì •
# =========================================================
st.set_page_config(
    page_title="ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ì±—ë´‡",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# =========================================================
# 1) CSS (ì›ë³¸ ìœ ì§€)
# =========================================================
st.markdown(
    """
<style>
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    .user-message {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #1976d2;
    }
    .assistant-message {
        background-color: #f5f5f5;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #424242;
    }
    .dataframe {
        font-size: 14px !important;
    }
    .status-box {
        background-color: #fff3e0;
        padding: 0.5rem 1rem;
        border-radius: 5px;
        border-left: 4px solid #ff9800;
        margin: 0.5rem 0;
    }
    .source-tag {
        background-color: #e8f5e9;
        padding: 0.2rem 0.5rem;
        border-radius: 3px;
        font-size: 0.85em;
        color: #2e7d32;
    }
    h1 {
        color: #1a237e;
    }
</style>
""",
    unsafe_allow_html=True,
)

# =========================================================
# 2) ìƒìˆ˜/ì„¤ì • (ì›ë³¸ ìœ ì§€)
# =========================================================
YEAR_TO_FILENAME = {
    2020: "2020ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°_ì‚¬ë³´ê³ ì„œ.pdf",
    2021: "2021ë…„_ìŠ¤ë§ˆíŠ¸_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³´ê³ ì„œ.pdf",
    2022: "2022ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³´ê³ ì„œ.pdf",
    2023: "2023ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´ì‹¤íƒœì¡°ì‚¬_ìµœì¢…ë³´ê³ ì„œ.pdf",
    2024: "2024_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³¸_ë³´ê³ ì„œ.pdf",
}
ALLOWED_FILES = list(YEAR_TO_FILENAME.values())

BOT_IDENTITY = """2020~2024ë…„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ì œê³µ ê°€ëŠ¥í•œ ì •ë³´:**
- ì—°ë„ë³„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ìœ„í—˜êµ° ë¹„ìœ¨ ë° ì¶”ì´
- ëŒ€ìƒë³„(ìœ ì•„ë™, ì²­ì†Œë…„, ì„±ì¸, 60ëŒ€) ê³¼ì˜ì¡´ í˜„í™©
- í•™ë ¹ë³„(ì´ˆ/ì¤‘/ê³ /ëŒ€í•™ìƒ) ì„¸ë¶€ ë¶„ì„
- ê³¼ì˜ì¡´ ê´€ë ¨ ìš”ì¸ ë¶„ì„ (SNS, ìˆí¼, ê²Œì„ ì´ìš© ë“±)
- ì¡°ì‚¬ ë°©ë²•ë¡  ë° í‘œë³¸ ì„¤ê³„ ì •ë³´
"""

# Hugging Face Dataset Repo (ì‚¬ìš©ì ê°’ ìœ ì§€)
HF_REPO_ID = "Rosaldowithbaek/smartphone-addiction-chroma-db"

# Streamlit Cloudì—ì„  ì‘ì—… ë””ë ‰í† ë¦¬ê°€ ë¦¬ì…‹ë  ìˆ˜ ìˆì–´ /tmp ê¶Œì¥ì„
# - ì›í•˜ì‹œë©´ "./chroma_db_store"ë¡œ ë°”ê¿”ë„ ë¨
LOCAL_DB_PATH = os.path.join(tempfile.gettempdir(), "chroma_db_store")

# ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (ì›ë³¸ ìœ ì§€)
N_QUERIES = 3
K_PER_QUERY = 6
TOP_PARENTS = 8
TOP_PARENTS_PER_FILE = 2
MAX_CHUNKS_PER_PARENT = 4
MAX_CHARS_PER_DOC = 8000
SUMMARY_TYPES = ["page_summary", "table_summary"]


# =========================================================
# 3) ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# - Streamlitì€ ì‚¬ìš©ì ì…ë ¥/ìƒí˜¸ì‘ìš©ë§ˆë‹¤ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‹¤í–‰(rerun)í•¨
# - rerunì—ë„ ìœ ì§€ë˜ì–´ì•¼ í•˜ëŠ” ê°’ì€ st.session_stateì— ì €ì¥í•´ì•¼ í•¨
# =========================================================
if "messages" not in st.session_state:
    st.session_state.messages = []  # í™”ë©´ í‘œì‹œìš© [{"role":"user/assistant","content":...}, ...]

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # LangChain ë©”ì‹œì§€ ê°ì²´ ë¦¬ìŠ¤íŠ¸ [HumanMessage, AIMessage, ...]

if "session_id" not in st.session_state:
    # ì‚¬ìš©ì ì„¸ì…˜ì„ ì‹ë³„í•˜ê¸° ìœ„í•œ ê°’(ëŒ€í™” ë¶„ë¦¬/ì²´í¬í¬ì¸í„° thread_idì— ì‚¬ìš©)
    st.session_state.session_id = str(uuid.uuid4())

if "_status_ph" not in st.session_state:
    # ë…¸ë“œ í•¨ìˆ˜ê°€ UI ìƒíƒœí‘œì‹œë¥¼ í•  ë•Œ ì°¸ì¡°í•˜ëŠ” placeholder ì €ì¥ì†Œ
    st.session_state._status_ph = None

if "graph" not in st.session_state:
    # ê·¸ë˜í”„ë¥¼ ì„¸ì…˜ë‹¹ 1íšŒë§Œ ìƒì„±í•´ì„œ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ê³µê°„
    st.session_state.graph = None


# =========================================================
# 4) LangGraph State ì •ì˜ (ì›ë³¸ ìœ ì§€)
# - TypedDict: dict í‚¤/íƒ€ì…ì„ ëª…ì‹œí•´ì„œ ì½”ë“œ ê°€ë…ì„±/ì•ˆì •ì„±ì„ ë†’ì´ëŠ” íƒ€ì… íŒíŠ¸ì„
# =========================================================
class GraphState(TypedDict):
    input: str
    chat_history: List[BaseMessage]
    session_id: str
    intent_raw: Optional[str]
    intent: Optional[str]
    is_chat_reference: Optional[bool]
    is_new_topic: Optional[bool]
    plan: Optional[Dict[str, Any]]
    resolved_question: Optional[str]
    previous_context: Optional[str]
    retrieval: Optional[Dict[str, Any]]
    context: Optional[str]
    draft_answer: Optional[str]
    validator_result: Optional[Dict[str, Any]]
    final_answer: Optional[str]
    debug_info: Optional[Dict[str, Any]]


# =========================================================
# 5) ìœ í‹¸: API Key ê°€ì ¸ì˜¤ê¸°
# - cacheë¡œ ë¬¶ìœ¼ë©´ "í‚¤ ì—†ëŠ” ìƒíƒœ"ê°€ ìºì‹œë¼ì„œ ë‚˜ì¤‘ì— í‚¤ ë„£ì–´ë„ ê³„ì† ì˜¤ë¥˜ ë‚˜ëŠ” ì¼€ì´ìŠ¤ ìˆìŒ
# - ê·¸ë˜ì„œ í‚¤ íƒìƒ‰ì€ ìºì‹±í•˜ì§€ ì•Šê³  ë§¤ rerunë§ˆë‹¤ ê°€ë³ê²Œ í™•ì¸í•˜ë„ë¡ í•¨
# =========================================================


def get_openai_api_key() -> Optional[str]:
    # 1) Streamlit secrets ìš°ì„ 
    try:
        key = st.secrets.get("OPENAI_API_KEY", None)
        if key:
            return str(key).strip()
    except Exception:
        pass

    # 2) í™˜ê²½ë³€ìˆ˜
    key = os.environ.get("OPENAI_API_KEY")
    if key:
        return str(key).strip()
    return None


# =========================================================
# 6) Hugging Faceì—ì„œ Chroma DB ë‹¤ìš´ë¡œë“œ (ìºì‹œë¨)
# - st.cache_resource: ë¬´ê±°ìš´ ë¦¬ì†ŒìŠ¤ë¥¼ "í•œ ë²ˆë§Œ" ë§Œë“¤ê³  ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ìºì‹œ
# =========================================================
@st.cache_resource(show_spinner=False)
def download_chroma_db(repo_id: str, local_dir: str) -> (Optional[str], Optional[str]):
    """
    HF Datasetì—ì„œ DB ìŠ¤ëƒ…ìƒ·ì„ ë‹¤ìš´ë¡œë“œ ë°›ì•„ local_dirì— ì €ì¥í•¨.
    ë°˜í™˜: (ë‹¤ìš´ë¡œë“œê²½ë¡œ or None, ì—ëŸ¬ë©”ì‹œì§€ or None)
    """
    # ì´ë¯¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¬ë‹¤ìš´ë¡œë“œ ìƒëµ
    if os.path.exists(local_dir) and os.listdir(local_dir):
        return local_dir, None

    try:
        from huggingface_hub import snapshot_download

        # snapshot_downloadëŠ” repo íŒŒì¼ ì „ì²´ë¥¼ local_dirë¡œ ë‚´ë ¤ë°›ìŒ
        snapshot_download(
            repo_id=repo_id,
            repo_type="dataset",
            local_dir=local_dir,
            local_dir_use_symlinks=False,
        )
        return local_dir, None

    except Exception as e:
        return None, str(e)


# =========================================================
# 7) ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” (ìºì‹œë¨)
# - api_key/db_pathë¥¼ ì¸ìë¡œ ë°›ì•„ ìºì‹œ í‚¤ì— í¬í•¨ì‹œí‚´(í‚¤ ë³€ê²½ ì‹œ ì¬ì´ˆê¸°í™” ê°€ëŠ¥)
# =========================================================
@st.cache_resource(show_spinner=False)
def init_resources(api_key: str, db_path: str):
    """
    vectorstore(Chroma) + llms(ChatOpenAIë“¤) ìƒì„±
    - embedding/Chroma ë¡œë”©ì€ ë¬´ê²ê¸° ë•Œë¬¸ì— ìºì‹œí•´ì„œ ì¬ì‚¬ìš©í•¨
    """
    os.environ["OPENAI_API_KEY"] = api_key

    # ì„ë² ë”© ëª¨ë¸(ì›ë³¸ ìœ ì§€)
    embedding = OpenAIEmbeddings(model="text-embedding-3-large")

    # Chroma ë¡œë”©(ì›ë³¸ ìœ ì§€)
    vectorstore = Chroma(
        persist_directory=db_path,
        embedding_function=embedding,
        collection_name="pdf_pages_with_summary_v2",
    )

    # LLM ì„¤ì •(ì›ë³¸ ìœ ì§€)
    llms = {
        "router": ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=10),
        "casual": ChatOpenAI(model="gpt-4o-mini", temperature=0.5, max_tokens=300),
        "main": ChatOpenAI(model="gpt-4o", temperature=0.2, max_tokens=3000),
        "planner": ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=800),
    }

    return vectorstore, llms


# =========================================================
# 8) í—¬í¼ í•¨ìˆ˜ë“¤ (ì›ë³¸ ìœ ì§€)
# =========================================================
def is_chat_reference_question(user_input: str) -> bool:
    patterns = [
        r"ë‚´\s*ì´ë¦„",
        r"ì œ\s*ì´ë¦„",
        r"ë‚˜(ë¥¼|ì˜|í•œí…Œ)",
        r"ë­ë¼ê³ \s*(í–ˆ|ë¬¼ì–´|ë§)",
        r"ì•„ê¹Œ",
        r"ë°©ê¸ˆ",
        r"ì´ì „ì—",
    ]
    for p in patterns:
        if re.search(p, user_input):
            return True
    return False


def is_new_topic_question(user_input: str, prev_keywords: List[str]) -> bool:
    followup_patterns = [
        r"^ê·¸ëŸ¬ë©´\s",
        r"^ê·¸ë˜ì„œ\s",
        r"^ê·¸ê±´\s",
        r"^ê·¸\s",
        r"ê²°ê³¼ëŠ”\s*\??$",
        r"ì–´ë•Œ\s*\??$",
        r"ì–´ë–»ê²Œ\s*(ë¼|ë˜)\s*\??$",
    ]
    for p in followup_patterns:
        if re.search(p, user_input):
            return False

    new_topic_keywords = ["ìˆí¼", "SNS", "ê²Œì„", "ì´ìš©ì‹œê°„", "ì´ìš©ë¥ ", "ê°€êµ¬ì›", "ì†Œë“", "ì§€ì—­", "ì„±ë³„", "ì—°ë ¹"]

    input_has_new_topic = any(kw in user_input for kw in new_topic_keywords)

    if input_has_new_topic:
        current_topics = [kw for kw in new_topic_keywords if kw in user_input]
        overlap = set(current_topics) & set(prev_keywords)
        if not overlap:
            return True

    if len(user_input) > 30 and not any(re.search(p, user_input) for p in followup_patterns):
        return True

    return False


def parse_year_range(text: str) -> List[int]:
    years = set()

    range_patterns = [
        r"(20[2][0-4])\s*ë…„?\s*(?:ì—ì„œ|ë¶€í„°|~|-|â€“)\s*(20[2][0-4])\s*ë…„?\s*(?:ê¹Œì§€)?",
        r"(20[2][0-4])\s*(?:~|-|â€“)\s*(20[2][0-4])",
    ]
    for pattern in range_patterns:
        matches = re.findall(pattern, text)
        for m in matches:
            start, end = int(m[0]), int(m[1])
            for y in range(start, end + 1):
                if y in YEAR_TO_FILENAME:
                    years.add(y)

    single_years = re.findall(r"\b(20[2][0-4])\s*ë…„?\b", text)
    for y in single_years:
        yi = int(y)
        if yi in YEAR_TO_FILENAME:
            years.add(yi)

    return sorted(list(years))


def extract_previous_context(chat_history: List[BaseMessage]) -> Dict[str, Any]:
    context = {"user_name": None, "last_topic": None, "last_years": [], "last_keywords": []}

    if not chat_history:
        return context

    # ì´ë¦„ ì°¾ê¸°
    for msg in chat_history:
        if isinstance(msg, HumanMessage):
            name_match = re.search(r"(?:ë‚´\s*ì´ë¦„ì€?|ì €ëŠ”?|ë‚˜ëŠ”?)\s*([ê°€-í£a-zA-Z]+)", msg.content)
            if name_match:
                context["user_name"] = name_match.group(1)

    # ìµœê·¼ 4ê°œ ë©”ì‹œì§€ì—ì„œ ë§¥ë½ ì¶”ì¶œ
    recent = chat_history[-4:] if len(chat_history) > 4 else chat_history

    for msg in reversed(recent):
        content = msg.content if hasattr(msg, "content") else str(msg)

        years = parse_year_range(content)
        if years and not context["last_years"]:
            context["last_years"] = years

        keywords = []
        kw_patterns = [
            r"(ê³¼ì˜ì¡´|ê³¼ì˜ì¡´ë¥ |ìœ„í—˜êµ°|ê³ ìœ„í—˜êµ°)",
            r"(ì²­ì†Œë…„|ìœ ì•„ë™|ì„±ì¸|60ëŒ€|ëŒ€í•™ìƒ|ì¤‘í•™ìƒ|ê³ ë“±í•™ìƒ|ì´ˆë“±í•™ìƒ|í•™ë ¹ë³„|ëŒ€ìƒë³„)",
            r"(SNS|ìˆí¼|ê²Œì„|ìœ íŠœë¸Œ|í‹±í†¡|ì¸ìŠ¤íƒ€)",
            r"(ì´ìš©ë¥ |ì´ìš©ì‹œê°„|ë¹„ìœ¨|ë³€í™”|ì¶”ì´)",
        ]
        for p in kw_patterns:
            found = re.findall(p, content)
            keywords.extend(found)

        if keywords and not context["last_keywords"]:
            context["last_keywords"] = list(set(keywords))

        if isinstance(msg, HumanMessage) and not context["last_topic"]:
            context["last_topic"] = content[:200]

    return context


def _keyword_boost_score(doc: Document, must_terms: List[str]) -> float:
    text = (doc.page_content or "").lower()
    text = re.sub(r"\s+", "", text)

    boost = 0.0
    for term in must_terms:
        term_norm = re.sub(r"\s+", "", term.lower())
        if term_norm in text:
            boost += 0.05
    return boost


# =========================================================
# 9) í…Œì´ë¸” íŒŒì‹±/ë Œë”ë§ (ì›ë³¸ ìœ ì§€)
# =========================================================
def parse_markdown_table(text: str) -> List[Dict[str, Any]]:
    tables = []
    lines = text.split("\n")

    i = 0
    while i < len(lines):
        line = lines[i].strip()

        if line.startswith("|") and line.endswith("|"):
            table_lines = []

            while i < len(lines):
                line = lines[i].strip()
                if line.startswith("|") and line.endswith("|"):
                    table_lines.append(line)
                    i += 1
                elif line.startswith("|---") or line.startswith("| ---"):
                    i += 1
                    continue
                else:
                    break

            if len(table_lines) >= 2:
                header_line = table_lines[0]
                headers = [h.strip() for h in header_line.split("|")[1:-1]]

                data_rows = []
                for row_line in table_lines[1:]:
                    if "---" in row_line:
                        continue
                    cells = [c.strip() for c in row_line.split("|")[1:-1]]
                    if len(cells) == len(headers):
                        data_rows.append(cells)

                if headers and data_rows:
                    tables.append({"headers": headers, "rows": data_rows, "start_idx": i - len(table_lines), "end_idx": i})
        else:
            i += 1

    return tables


def render_table(headers: List[str], rows: List[List[str]]) -> None:
    try:
        df = pd.DataFrame(rows, columns=headers)
        st.dataframe(df, use_container_width=True, hide_index=True)
    except Exception:
        st.markdown("| " + " | ".join(headers) + " |")
        st.markdown("| " + " | ".join(["---"] * len(headers)) + " |")
        for row in rows:
            st.markdown("| " + " | ".join(row) + " |")


def render_answer_with_tables(answer: str) -> None:
    tables = parse_markdown_table(answer)

    if not tables:
        st.markdown(answer)
        return

    lines = answer.split("\n")
    current_pos = 0

    for table in tables:
        before_text = "\n".join(lines[current_pos : table["start_idx"]])
        if before_text.strip():
            st.markdown(before_text)

        render_table(table["headers"], table["rows"])
        current_pos = table["end_idx"]

    after_text = "\n".join(lines[current_pos:])
    if after_text.strip():
        st.markdown(after_text)


# =========================================================
# 10) í”„ë¡¬í”„íŠ¸ íŒ©í† ë¦¬ (ì›ë³¸ ìœ ì§€)
# =========================================================
def get_router_prompt():
    return ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.\n"
                "ì´ ì‹œìŠ¤í…œì€ 'ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024)' ì „ë¬¸ RAGì…ë‹ˆë‹¤.\n\n"
                "ë¶„ë¥˜ ê¸°ì¤€:\n"
                "SMALLTALK: ì¸ì‚¬, ê°ì‚¬, ì¡ë‹´, ì‹œìŠ¤í…œ ì†Œê°œ ìš”ì²­\n"
                "RAG: ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì¡°ì‚¬ ê´€ë ¨ ì§ˆë¬¸\n"
                "CHAT_REF: ì´ì „ ëŒ€í™” ë‚´ìš© ì°¸ì¡°\n"
                "OFFTOPIC: ì™„ì „íˆ ê´€ë ¨ ì—†ëŠ” ì£¼ì œ\n\n"
                "ì¶œë ¥: SMALLTALK / RAG / CHAT_REF / OFFTOPIC ì¤‘ í•˜ë‚˜ë§Œ",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )


def get_smalltalk_prompt():
    return ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
                f"ì‹œìŠ¤í…œ ì—­í• :\n{BOT_IDENTITY}\n\n"
                "ì‘ë‹µ ì§€ì¹¨:\n"
                "- ì¸ì‚¬ì—ëŠ” ê°„ê²°í•˜ê²Œ ì‘ëŒ€í•˜ê³  ì‹œìŠ¤í…œ ì—­í• ì„ ì•ˆë‚´\n"
                "- ì´ëª¨í‹°ì½˜ ì‚¬ìš© ê¸ˆì§€\n"
                "- ê²©ì‹ì²´ ì‚¬ìš© (ìŠµë‹ˆë‹¤/ì…ë‹ˆë‹¤)\n"
                "- 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )


def get_offtopic_prompt():
    return ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
                f"ì‹œìŠ¤í…œ ì—­í• :\n{BOT_IDENTITY}\n\n"
                "ë„ë©”ì¸ ì™¸ ì§ˆë¬¸ ì‘ëŒ€:\n"
                "- í•´ë‹¹ ì£¼ì œëŠ” ì „ë¬¸ ë¶„ì•¼ê°€ ì•„ë‹˜ì„ ì•ˆë‚´\n"
                "- ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ê´€ë ¨ ì§ˆë¬¸ì€ ë„ì›€ ê°€ëŠ¥í•¨ì„ ì–¸ê¸‰\n"
                "- ê²©ì‹ì²´ ì‚¬ìš©, 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )


def get_planner_prompt():
    return ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ê¸°ì…ë‹ˆë‹¤.\n"
                "ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.\n\n"
                "ì„ë¬´:\n"
                "1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ìê¸°ì™„ê²°í˜•ìœ¼ë¡œ ì¬êµ¬ì„±\n"
                "2. ê²€ìƒ‰ ì¿¼ë¦¬ 3ê°œ ìƒì„±\n"
                "3. í•„ìš”í•œ ì—°ë„/íŒŒì¼ ì‹ë³„\n\n"
                "ìƒˆ ì£¼ì œ vs í›„ì†ì§ˆë¬¸ íŒë‹¨:\n"
                "- is_new_topic=true: ì´ì „ ë§¥ë½ ë¬´ì‹œ\n"
                "- is_new_topic=false: ì´ì „ ë§¥ë½ í™œìš©\n\n"
                "ì—°ë„ ë²”ìœ„ ì²˜ë¦¬:\n"
                "- '2021ë…„ì—ì„œ 2024ë…„ê¹Œì§€' â†’ years: [2021, 2022, 2023, 2024]\n\n"
                "í—ˆìš© íŒŒì¼ëª…:\n"
                + "\n".join([f"- {y}ë…„: {fn}" for y, fn in YEAR_TO_FILENAME.items()])
                + "\n\nJSON ìŠ¤í‚¤ë§ˆ:\n"
                "{\n"
                '  "resolved_question": "ì™„ì „í•œ ì§ˆë¬¸",\n'
                '  "years": [2020, ...],\n'
                '  "file_name_filters": ["íŒŒì¼ëª…"],\n'
                '  "query_type": "ì¡°ì‚¬ì„¤ê³„" | "ê²°ê³¼/ë¶„ì„",\n'
                '  "must_keep_terms": ["í•µì‹¬ìš©ì–´"],\n'
                '  "queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]\n'
                "}",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "í˜„ì¬ ì§ˆë¬¸: {input}\nìƒˆ ì£¼ì œ ì—¬ë¶€: {is_new_topic}\nì´ì „ ë§¥ë½: {prev_context}\n\nJSON:"),
        ]
    )


def get_answer_prompt():
    return ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
                "í•µì‹¬ ì›ì¹™:\n"
                "1. CONTEXTì— ìˆëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜/ë¹„ìœ¨ì„ ë°˜ë“œì‹œ ì¸ìš©\n"
                "2. ëª¨ë“  ìˆ˜ì¹˜ì—ëŠ” ì¶œì²˜(íŒŒì¼ëª… p.í˜ì´ì§€) í•„ìˆ˜\n"
                "3. ì—°ë„ë³„ ë¹„êµ ì‹œ ë³€í™”ëŸ‰(%p) ëª…ì‹œ\n"
                "4. ê°ê´€ì ì´ê³  ë‹´ë°±í•œ í†¤ ìœ ì§€\n\n"
                "í˜•ì‹ ê·œì¹™:\n"
                "- í•µì‹¬ ìˆ˜ì¹˜ë¥¼ ë¨¼ì € ì œì‹œ\n"
                "- ì—°ë„ë³„ ë°ì´í„°ëŠ” ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ ì‚¬ìš©\n"
                "- ì´ëª¨í‹°ì½˜ ì‚¬ìš© ê¸ˆì§€\n"
                "- ê²©ì‹ì²´ ì‚¬ìš©\n\n"
                "ì£¼ì˜:\n"
                "- CONTEXTì— ì—†ëŠ” ì—°ë„ëŠ” 'í•´ë‹¹ ì—°ë„ ë°ì´í„°ëŠ” ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'ë¡œ ëª…ì‹œ\n"
                "- ì¶”ì¸¡í•˜ì§€ ì•Šê³  ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€",
            ),
            ("human", "[ì§ˆë¬¸]\n{input}\n\n[ê²€ìƒ‰ ê²°ê³¼]\n{context}\n\nìœ„ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤."),
        ]
    )


def get_validator_prompt():
    return ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "í†µê³„ ë³´ê³ ì„œ ë‹µë³€ í’ˆì§ˆ ê²€ìˆ˜ê¸°ì…ë‹ˆë‹¤.\n\n"
                "ê²€ìˆ˜ í•­ëª©:\n"
                "1. ìˆ˜ì¹˜/ë¹„ìœ¨ì— ì¶œì²˜ ìˆëŠ”ì§€\n"
                "2. CONTEXTì— ì—†ëŠ” ìˆ˜ì¹˜ë¥¼ ìƒì„±í–ˆëŠ”ì§€\n"
                "3. ì§ˆë¬¸ì—ì„œ ìš”ì²­í•œ ì—°ë„/í•­ëª©ì„ ëª¨ë‘ ë‹¤ë¤˜ëŠ”ì§€\n\n"
                "JSONë§Œ ì¶œë ¥:\n"
                "{\n"
                '  "needs_fix": true|false,\n'
                '  "issues": ["ë¬¸ì œì "],\n'
                '  "corrected_answer": "ìˆ˜ì •ëœ ë‹µë³€ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´"\n'
                "}",
            ),
            ("human", "[ì§ˆë¬¸]\n{input}\n\n[ê²€ìƒ‰ ê²°ê³¼]\n{context}\n\n[ë‹µë³€]\n{answer}\n\nJSON:"),
        ]
    )


# =========================================================
# 11) ë…¸ë“œ í•¨ìˆ˜ ìƒì„±
# - ì„±ëŠ¥ í•µì‹¬ ë³€ê²½ì :
#   * status_placeholderë¥¼ í•¨ìˆ˜ í´ë¡œì €ë¡œ ì¡ì§€ ì•ŠìŒ
#   * ë…¸ë“œ ë‚´ë¶€ì—ì„œ st.session_state._status_phë¥¼ ì°¸ì¡°í•¨
#   => ê·¸ë˜í”„ë¥¼ ì„¸ì…˜ 1íšŒ ìƒì„±í•´ë„ UI ì—…ë°ì´íŠ¸ê°€ ê°€ëŠ¥í•´ì§
# =========================================================
def create_node_functions(vectorstore: Chroma, llms: Dict[str, ChatOpenAI]):
    # --- ìƒíƒœ UI ì—…ë°ì´íŠ¸ í•¨ìˆ˜ ---
    def update_status(message: str):
        ph = st.session_state.get("_status_ph")
        if ph is None:
            return
        ph.markdown(
            f"""
        <div style="background-color: #fff3e0; padding: 0.8rem 1rem; border-radius: 8px; 
                    border-left: 4px solid #ff9800; margin: 0.5rem 0;">
            <span style="font-weight: 500;">ğŸ”„ {message}</span>
        </div>
        """,
            unsafe_allow_html=True,
        )

    # --- ë…¸ë“œ1: ë¼ìš°í„° ---
    def route_intent(state: GraphState) -> GraphState:
        update_status("ì§ˆë¬¸ ë¶„ì„ ì¤‘...")

        try:
            user_input = state["input"]
            chat_history = state.get("chat_history", [])

            # 1) ëŒ€í™” ì°¸ì¡° ì§ˆë¬¸ ë¨¼ì € ê°ì§€
            if is_chat_reference_question(user_input):
                state["intent_raw"] = "CHAT_REF"
                state["intent"] = "CHAT_REF"
                state["is_chat_reference"] = True
                return state

            # 2) ìƒˆ ì£¼ì œ ì—¬ë¶€ íŒë‹¨
            prev_ctx = extract_previous_context(chat_history)
            state["is_new_topic"] = is_new_topic_question(user_input, prev_ctx.get("last_keywords", []))

            # 3) LLM ë¼ìš°í„°
            result = (get_router_prompt() | llms["router"] | StrOutputParser()).invoke(
                {"input": user_input, "chat_history": chat_history}
            )
            state["intent_raw"] = result.strip().upper()

            # 4) ê°€ë“œ: ì—°ë„/í‚¤ì›Œë“œ ê¸°ë°˜ ë³´ì •(ì›ë³¸ ìœ ì§€)
            if re.search(r"\b(20[2][0-4])\s*ë…„?\b", user_input):
                state["intent"] = "RAG"
                return state

            rag_keywords = [
                "ê³¼ì˜ì¡´",
                "ìŠ¤ë§ˆíŠ¸í°",
                "ì¡°ì‚¬",
                "ì‹¤íƒœ",
                "ë¹„ìœ¨",
                "ë¥ ",
                "%",
                "í†µê³„",
                "ìˆ˜ì¹˜",
                "ê²°ê³¼",
                "ì²­ì†Œë…„",
                "ëŒ€í•™ìƒ",
                "ì„±ì¸",
                "ìˆí¼",
                "SNS",
                "ê²Œì„",
                "ì´ìš©ë¥ ",
                "ìœ„í—˜êµ°",
            ]
            if any(kw in user_input for kw in rag_keywords):
                state["intent"] = "RAG"
                return state

            # 5) ê¸°ë³¸: raw ê²°ê³¼ ì‹ ë¢°
            if state["intent_raw"] in ("SMALLTALK", "RAG", "OFFTOPIC", "CHAT_REF"):
                state["intent"] = state["intent_raw"]
            else:
                state["intent"] = "RAG"

            return state

        except Exception:
            state["intent"] = "RAG"
            return state

    # --- ë…¸ë“œ2: smalltalk ---
    def handle_smalltalk(state: GraphState) -> GraphState:
        update_status("ì‘ë‹µ ìƒì„± ì¤‘...")
        try:
            answer = (get_smalltalk_prompt() | llms["casual"] | StrOutputParser()).invoke(
                {"input": state["input"], "chat_history": state.get("chat_history", [])}
            )
            state["final_answer"] = answer
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state

    # --- ë…¸ë“œ2: offtopic ---
    def handle_offtopic(state: GraphState) -> GraphState:
        update_status("ì‘ë‹µ ìƒì„± ì¤‘...")
        try:
            answer = (get_offtopic_prompt() | llms["casual"] | StrOutputParser()).invoke(
                {"input": state["input"], "chat_history": state.get("chat_history", [])}
            )
            state["final_answer"] = answer
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state

    # --- ë…¸ë“œ2: chat_ref ---
    def handle_chat_reference(state: GraphState) -> GraphState:
        update_status("ëŒ€í™” ê¸°ë¡ í™•ì¸ ì¤‘...")
        try:
            chat_history = state.get("chat_history", [])
            user_input = state["input"]
            prev_ctx = extract_previous_context(chat_history)

            if re.search(r"(ë‚´|ì œ)\s*ì´ë¦„", user_input):
                if prev_ctx["user_name"]:
                    state["final_answer"] = f"{prev_ctx['user_name']}ë‹˜ìœ¼ë¡œ ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤."
                else:
                    state["final_answer"] = "ì•„ì§ ì´ë¦„ì„ ë§ì”€í•´ì£¼ì‹œì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                return state

            if re.search(r"(ë­ë¼ê³ |ë¬´ìŠ¨\s*ë§|ë­\s*ë¬¼ì–´)", user_input):
                if prev_ctx["last_topic"]:
                    state["final_answer"] = f"ì´ì „ì— '{prev_ctx['last_topic'][:80]}...'ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì…¨ìŠµë‹ˆë‹¤."
                else:
                    state["final_answer"] = "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                return state

            state["final_answer"] = "ì´ì „ ëŒ€í™” ì°¸ì¡°ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
            return state

        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state

    # --- ë…¸ë“œ3: í”Œë˜ë„ˆ ---
    def plan_search(state: GraphState) -> GraphState:
        update_status("ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ ì¤‘...")

        try:
            user_input = state["input"]
            chat_history = state.get("chat_history", [])
            is_new_topic = state.get("is_new_topic", True)

            prev_ctx = extract_previous_context(chat_history)

            if is_new_topic:
                prev_context_str = "ìƒˆë¡œìš´ ì£¼ì œ - ì´ì „ ë§¥ë½ ë¬´ì‹œ"
            else:
                prev_context_str = ""
                if prev_ctx["last_topic"]:
                    prev_context_str += f"ì´ì „ ì£¼ì œ: {prev_ctx['last_topic'][:100]}\n"
                if prev_ctx["last_years"]:
                    prev_context_str += f"ì´ì „ ì—°ë„: {prev_ctx['last_years']}\n"
                if prev_ctx["last_keywords"]:
                    prev_context_str += f"ì´ì „ í‚¤ì›Œë“œ: {prev_ctx['last_keywords']}"
                if not prev_context_str:
                    prev_context_str = "ì—†ìŒ"

            state["previous_context"] = prev_context_str

            result = (get_planner_prompt() | llms["planner"] | StrOutputParser()).invoke(
                {
                    "input": user_input,
                    "chat_history": chat_history,
                    "is_new_topic": str(is_new_topic),
                    "prev_context": prev_context_str,
                }
            )

            # LLMì´ JSON ì•ë’¤ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì„ì„ ìˆ˜ ìˆì–´ì„œ JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
            json_match = re.search(r"\{[\s\S]*\}", result)
            if json_match:
                result = json_match.group()

            plan = json.loads(result)

            years = plan.get("years", [])
            if not isinstance(years, list):
                years = []

            # ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì—°ë„ ì¶”ì¶œ ë³´ê°•
            input_years = parse_year_range(user_input)
            years = list(set(years + input_years))
            years = [y for y in years if isinstance(y, int) and y in YEAR_TO_FILENAME]
            years = sorted(years)

            # í›„ì†ì§ˆë¬¸ì¸ë° ì—°ë„ ì—†ìœ¼ë©´ ì´ì „ ì—°ë„ ì‚¬ìš©
            if not years and not is_new_topic and prev_ctx["last_years"]:
                years = prev_ctx["last_years"]

            fns = plan.get("file_name_filters", [])
            if not isinstance(fns, list):
                fns = []
            fns = [fn for fn in fns if isinstance(fn, str) and fn in ALLOWED_FILES]

            # ì—°ë„ -> íŒŒì¼ëª… ë§¤í•‘
            if years and not fns:
                fns = [YEAR_TO_FILENAME[y] for y in years if y in YEAR_TO_FILENAME]

            queries = plan.get("queries", [])
            if not isinstance(queries, list):
                queries = []
            queries = [str(q).strip() for q in queries if str(q).strip()]

            resolved_q = plan.get("resolved_question", "")
            if not isinstance(resolved_q, str):
                resolved_q = ""
            resolved_q = resolved_q.strip()

            if len(resolved_q) < 15 and not is_new_topic and prev_ctx["last_keywords"]:
                keywords_str = " ".join(prev_ctx["last_keywords"])
                resolved_q = f"{keywords_str} {resolved_q}".strip()

            fallback_q = resolved_q or user_input

            # ì¿¼ë¦¬ ê°œìˆ˜ ë³´ì •
            while len(queries) < N_QUERIES:
                queries.append(fallback_q)
            if len(queries) > N_QUERIES:
                queries = queries[:N_QUERIES]

            keep = plan.get("must_keep_terms", [])
            if not isinstance(keep, list):
                keep = []
            keep = [str(x).strip() for x in keep if str(x).strip()]

            if not is_new_topic and prev_ctx["last_keywords"]:
                keep = list(set(keep + prev_ctx["last_keywords"]))

            state["plan"] = {
                "years": years,
                "file_name_filters": fns,
                "query_type": plan.get("query_type", "ê²°ê³¼/ë¶„ì„"),
                "must_keep_terms": keep,
                "queries": queries,
                "resolved_question": resolved_q,
            }
            state["resolved_question"] = resolved_q
            return state

        except Exception:
            # ì‹¤íŒ¨ ì‹œ í´ë°±(ì›ë³¸ ë¡œì§ ìœ ì§€)
            is_new_topic = state.get("is_new_topic", True)
            prev_ctx = extract_previous_context(state.get("chat_history", []))
            fallback_years = parse_year_range(state["input"])

            if not fallback_years and not is_new_topic and prev_ctx["last_years"]:
                fallback_years = prev_ctx["last_years"]

            fallback_fns = [YEAR_TO_FILENAME[y] for y in fallback_years if y in YEAR_TO_FILENAME]
            resolved = state["input"]

            state["plan"] = {
                "years": fallback_years,
                "file_name_filters": fallback_fns,
                "query_type": "ê²°ê³¼/ë¶„ì„",
                "must_keep_terms": [] if is_new_topic else prev_ctx.get("last_keywords", []),
                "queries": [resolved] * N_QUERIES,
                "resolved_question": resolved,
            }
            state["resolved_question"] = resolved
            return state

    # --- ê²€ìƒ‰ í˜¸ì¶œì„ "ê°€ëŠ¥í•˜ë©´" ì„ë² ë”© ì¤‘ë³µ ì—†ì´ ì²˜ë¦¬(ì§€ì› ì•ˆ ë˜ë©´ ì›ë³¸ ë°©ì‹ fallback) ---
    def _search_with_best_effort(query: str, k: int, flt: dict):
        """
        ëª©ì : ì¿¼ë¦¬ ì„ë² ë”© ì¤‘ë³µì„ ì¤„ì—¬ ì„±ëŠ¥ ê°œì„ 
        - Chroma ë˜í¼ ë²„ì „ì— ë”°ë¼ ë²¡í„° ê²€ìƒ‰ ë©”ì„œë“œê°€ ì—†ì„ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ fallback ì²˜ë¦¬
        """
        # 1) vector ê¸°ë°˜ ê²€ìƒ‰ ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš© ì‹œë„
        #    (ë©”ì„œë“œëª…ì€ í™˜ê²½/ë²„ì „ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ hasattrë¡œ ë°©ì–´)
        try:
            embed_fn = getattr(vectorstore, "_embedding_function", None)
            if embed_fn is not None and hasattr(embed_fn, "embed_query"):
                q_vec = embed_fn.embed_query(query)

                # ì•„ë˜ ë©”ì„œë“œê°€ ì¡´ì¬í•˜ë©´ ìš°ì„  ì‚¬ìš©
                if hasattr(vectorstore, "similarity_search_by_vector_with_relevance_scores"):
                    return vectorstore.similarity_search_by_vector_with_relevance_scores(q_vec, k=k, filter=flt)

                if hasattr(vectorstore, "similarity_search_by_vector"):
                    # relevance scoreê°€ ì—†ìœ¼ë©´ docë§Œ ë°˜í™˜ë  ìˆ˜ ìˆìŒ -> scoreë¥¼ 0ìœ¼ë¡œ ì±„ì›Œ í˜•íƒœ ë§ì¶¤
                    docs = vectorstore.similarity_search_by_vector(q_vec, k=k, filter=flt)
                    return [(d, 0.0) for d in docs]
        except Exception:
            pass

        # 2) fallback: ì›ë³¸ ë°©ì‹(ë‚´ë¶€ì—ì„œ ì„ë² ë”©ì´ ë§¤ë²ˆ ìƒì„±ë  ìˆ˜ ìˆìŒ)
        return vectorstore.similarity_search_with_relevance_scores(query, k=k, filter=flt)

    # --- ë…¸ë“œ4: ê²€ìƒ‰ ---
    def retrieve_documents(state: GraphState) -> GraphState:
        update_status("ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘...")

        try:
            plan = state["plan"]
            target_files = plan.get("file_name_filters", [])
            queries = plan.get("queries", [])
            must_terms = plan.get("must_keep_terms", [])

            all_docs: List[Document] = []

            if target_files:
                # ë©€í‹°ì—°ë„: íŒŒì¼ë³„ë¡œ ê· ë“±í•˜ê²Œ ìˆ˜ì§‘(ì›ë³¸ ìœ ì§€)
                for fn in target_files:
                    file_filter = {"$and": [{"doc_type": {"$in": SUMMARY_TYPES}}, {"file_name": fn}]}

                    file_docs = []
                    seen_keys = set()

                    for q in queries:
                        if not q:
                            continue
                        hits = _search_with_best_effort(q, k=K_PER_QUERY, flt=file_filter)
                        for doc, score in hits:
                            key = f"{doc.metadata.get('parent_id')}|{doc.metadata.get('page')}"
                            if key in seen_keys:
                                continue
                            doc.metadata["_score"] = float(score)
                            doc.metadata["_source_file"] = fn
                            file_docs.append(doc)
                            seen_keys.add(key)

                    # í‚¤ì›Œë“œ ë¶€ìŠ¤íŠ¸(ì›ë³¸ ìœ ì§€)
                    for doc in file_docs:
                        base_score = doc.metadata.get("_score", 0.0)
                        boost = _keyword_boost_score(doc, must_terms)
                        doc.metadata["_final_score"] = base_score + boost

                    file_docs.sort(key=lambda d: d.metadata.get("_final_score", 0.0), reverse=True)
                    all_docs.extend(file_docs[: TOP_PARENTS_PER_FILE * 2])
            else:
                base_filter = {"doc_type": {"$in": SUMMARY_TYPES}}
                seen_keys = set()

                for q in queries:
                    if not q:
                        continue
                    hits = _search_with_best_effort(q, k=K_PER_QUERY, flt=base_filter)
                    for doc, score in hits:
                        key = f"{doc.metadata.get('parent_id')}|{doc.metadata.get('page')}"
                        if key in seen_keys:
                            continue
                        doc.metadata["_score"] = float(score)
                        all_docs.append(doc)
                        seen_keys.add(key)

                for doc in all_docs:
                    base_score = doc.metadata.get("_score", 0.0)
                    boost = _keyword_boost_score(doc, must_terms)
                    doc.metadata["_final_score"] = base_score + boost

            all_docs.sort(key=lambda d: d.metadata.get("_final_score", 0.0), reverse=True)

            # Parent ì„ ì •(ì›ë³¸ ìœ ì§€)
            parent_ids = []
            seen_pid = set()

            if target_files:
                for fn in target_files:
                    for doc in all_docs:
                        if doc.metadata.get("file_name") != fn:
                            continue
                        pid = doc.metadata.get("parent_id")
                        if pid and pid not in seen_pid:
                            parent_ids.append(pid)
                            seen_pid.add(pid)
                            break

                for doc in all_docs:
                    if len(parent_ids) >= TOP_PARENTS:
                        break
                    pid = doc.metadata.get("parent_id")
                    if pid and pid not in seen_pid:
                        parent_ids.append(pid)
                        seen_pid.add(pid)
            else:
                for doc in all_docs:
                    pid = doc.metadata.get("parent_id")
                    if not pid or pid in seen_pid:
                        continue
                    parent_ids.append(pid)
                    seen_pid.add(pid)
                    if len(parent_ids) >= TOP_PARENTS:
                        break

            # text_chunk í™•ì¥(ì›ë³¸ ìœ ì§€)
            expanded_chunks = []
            for pid in parent_ids:
                got = vectorstore._collection.get(where={"parent_id": pid}, include=["documents", "metadatas"])
                docs = got.get("documents", []) or []
                metas = got.get("metadatas", []) or []

                chunks = []
                for txt, meta in zip(docs, metas):
                    if not isinstance(meta, dict):
                        continue
                    if meta.get("doc_type") != "text_chunk":
                        continue
                    idx = int(meta.get("chunk_index", 0))
                    chunks.append((idx, txt or "", meta))

                chunks.sort(key=lambda x: x[0])
                for idx, txt, meta in chunks[:MAX_CHUNKS_PER_PARENT]:
                    expanded_chunks.append(Document(page_content=txt, metadata=meta))

            pid_set = set(parent_ids)
            kept_summaries = [d for d in all_docs if d.metadata.get("parent_id") in pid_set]
            final_docs = kept_summaries + expanded_chunks

            # CONTEXT êµ¬ì„±(ì›ë³¸ ìœ ì§€)
            blocks = []
            for i, d in enumerate(final_docs, start=1):
                m = d.metadata
                text = (d.page_content or "")[:MAX_CHARS_PER_DOC]
                blocks.append(f"[{i}] {m.get('file_name', 'unknown')} (p.{m.get('page', '?')})\n{text}")
            context = "\n\n---\n\n".join(blocks)

            state["retrieval"] = {"docs": final_docs, "parent_ids": parent_ids, "files_searched": target_files or ["ì „ì²´"]}
            state["context"] = context
            return state

        except Exception:
            state["context"] = ""
            return state

    # --- ë…¸ë“œ5: ë‹µë³€ ìƒì„± ---
    def generate_answer(state: GraphState) -> GraphState:
        update_status("ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            answer = (get_answer_prompt() | llms["main"] | StrOutputParser()).invoke(
                {"input": state["resolved_question"] or state["input"], "context": state.get("context", "")}
            )
            state["draft_answer"] = answer
            return state
        except Exception as e:
            state["draft_answer"] = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state

    # --- ë…¸ë“œ6: ê²€ì¦ ---
    def validate_answer(state: GraphState) -> GraphState:
        update_status("ë‹µë³€ ê²€ì¦ ì¤‘...")

        try:
            result = (get_validator_prompt() | llms["main"] | StrOutputParser()).invoke(
                {"input": state["resolved_question"] or state["input"], "context": state.get("context", ""), "answer": state["draft_answer"]}
            )

            json_match = re.search(r"\{[\s\S]*\}", result)
            if json_match:
                result = json_match.group()

            validator_out = json.loads(result)
            state["validator_result"] = validator_out

            if validator_out.get("needs_fix") and validator_out.get("corrected_answer"):
                state["final_answer"] = validator_out["corrected_answer"]
            else:
                state["final_answer"] = state["draft_answer"]

            return state

        except Exception:
            state["final_answer"] = state.get("draft_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return state

    # --- clarify ---
    def handle_clarify(state: GraphState) -> GraphState:
        clarify_msg = (state.get("resolved_question") or "").replace("CLARIFY:", "", 1).strip()
        state["final_answer"] = clarify_msg
        return state

    return {
        "route_intent": route_intent,
        "smalltalk": handle_smalltalk,
        "offtopic": handle_offtopic,
        "chat_ref": handle_chat_reference,
        "plan_search": plan_search,
        "retrieve": retrieve_documents,
        "generate": generate_answer,
        "validate": validate_answer,
        "clarify": handle_clarify,
    }


# =========================================================
# 12) ê·¸ë˜í”„ ë¹Œë” (ì›ë³¸ ìœ ì§€)
# - í•µì‹¬ ë³€ê²½ì : ì´ ê·¸ë˜í”„ë¥¼ "ë§¤ ì§ˆë¬¸ë§ˆë‹¤" ë§Œë“¤ì§€ ì•Šê³  ì„¸ì…˜ë‹¹ 1íšŒë§Œ ìƒì„±/ì¬ì‚¬ìš©í•¨
# =========================================================
def build_graph(node_functions):
    workflow = StateGraph(GraphState)

    for name, func in node_functions.items():
        workflow.add_node(name, func)

    def route_by_intent(state: GraphState) -> str:
        intent = state.get("intent", "RAG")
        if intent == "SMALLTALK":
            return "smalltalk"
        elif intent == "OFFTOPIC":
            return "offtopic"
        elif intent == "CHAT_REF":
            return "chat_ref"
        else:
            return "rag_pipeline"

    def check_clarify(state: GraphState) -> str:
        resolved = state.get("resolved_question", "") or ""
        if resolved.startswith("CLARIFY:"):
            return "clarify"
        return "retrieve"

    workflow.set_entry_point("route_intent")

    workflow.add_conditional_edges(
        "route_intent",
        route_by_intent,
        {"smalltalk": "smalltalk", "offtopic": "offtopic", "chat_ref": "chat_ref", "rag_pipeline": "plan_search"},
    )

    workflow.add_edge("smalltalk", END)
    workflow.add_edge("offtopic", END)
    workflow.add_edge("chat_ref", END)

    workflow.add_conditional_edges("plan_search", check_clarify, {"clarify": "clarify", "retrieve": "retrieve"})

    workflow.add_edge("clarify", END)
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", "validate")
    workflow.add_edge("validate", END)

    # MemorySaver: ê·¸ë˜í”„ ë‚´ ìƒíƒœ ì €ì¥(ì„¸ì…˜ ê³ ì • ì‹œ ì˜ë¯¸ê°€ ìƒê¹€)
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)


# =========================================================
# 13) ë©”ì¸ UI
# =========================================================
def main():
    st.title("ğŸ“Š ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë¶„ì„ ì‹œìŠ¤í…œ")

    # --- ì‚¬ì´ë“œë°” ---
    with st.sidebar:
        st.header("ì‹œìŠ¤í…œ ì •ë³´")
        st.markdown(BOT_IDENTITY)

        st.divider()

        st.subheader("ë°ì´í„° ë²”ìœ„")
        for year in YEAR_TO_FILENAME.keys():
            st.caption(f"â€¢ {year}ë…„")

        st.divider()

        debug_mode = st.checkbox("ë””ë²„ê·¸ ëª¨ë“œ", value=False)

        # ëŒ€í™” ì´ˆê¸°í™”(ê·¸ë˜í”„/ì²´í¬í¬ì¸í„°ë„ í•¨ê»˜ ë¦¬ì…‹ ê¶Œì¥)
        if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.session_state.chat_history = []
            st.session_state.session_id = str(uuid.uuid4())
            st.session_state.graph = None
            st.rerun()

        st.divider()
        st.caption(f"DB ê²½ë¡œ: {LOCAL_DB_PATH}")
        st.caption(f"HF Repo: {HF_REPO_ID}")

    # =========================================================
    # 1) DB ë‹¤ìš´ë¡œë“œ (í•„ìš” ì‹œ)
    # =========================================================
    if not os.path.exists(LOCAL_DB_PATH) or not os.listdir(LOCAL_DB_PATH):
        st.info("ğŸ”„ Chroma DBë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")

        with st.spinner("Hugging Faceì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            db_path, error = download_chroma_db(HF_REPO_ID, LOCAL_DB_PATH)

        if error:
            st.error(f"DB ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error}")
            st.info("HF_REPO_ID / repo_type / ê¶Œí•œ(Privateë©´ í† í°) ë“±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return

        st.success("DB ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        st.rerun()

    # =========================================================
    # 2) API Key í™•ì¸
    # =========================================================
    api_key = get_openai_api_key()
    if not api_key:
        st.error("ì´ˆê¸°í™” ì˜¤ë¥˜: OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.info("Streamlit Community Cloudì˜ Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        with st.form("api_key_form"):
            entered = st.text_input("OpenAI API í‚¤", type="password")
            submitted = st.form_submit_button("ì„¤ì •")
            if submitted and entered:
                os.environ["OPENAI_API_KEY"] = entered.strip()
                st.rerun()
        return

    # =========================================================
    # 3) ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”(ìºì‹œ)
    # =========================================================
    try:
        vectorstore, llms = init_resources(api_key=api_key, db_path=LOCAL_DB_PATH)
    except Exception as e:
        st.error(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return

    # =========================================================
    # 4) ê·¸ë˜í”„ ì„¸ì…˜ 1íšŒ ìƒì„±(í•µì‹¬ ì„±ëŠ¥ ê°œì„ )
    # =========================================================
    if st.session_state.graph is None:
        node_functions = create_node_functions(vectorstore, llms)
        st.session_state.graph = build_graph(node_functions)

    graph = st.session_state.graph

    # =========================================================
    # 5) ê¸°ì¡´ ë©”ì‹œì§€ ë Œë”ë§
    # =========================================================
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                render_answer_with_tables(message["content"])
            else:
                st.markdown(message["content"])

    # =========================================================
    # 6) ì…ë ¥ ì²˜ë¦¬
    # =========================================================
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ìœ ì € ë©”ì‹œì§€ ì €ì¥/í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            status_placeholder = st.empty()
            answer_placeholder = st.empty()

            # ë…¸ë“œ í•¨ìˆ˜ê°€ ìƒíƒœ UI ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆë„ë¡ ì„¸ì…˜ì— placeholder ì£¼ì…
            st.session_state._status_ph = status_placeholder

            try:
                config = {"configurable": {"thread_id": st.session_state.session_id}}

                result = graph.invoke(
                    {"input": prompt, "chat_history": st.session_state.chat_history, "session_id": st.session_state.session_id},
                    config=config,
                )

                # ìƒíƒœ ë°•ìŠ¤ ì œê±°
                status_placeholder.empty()

                final_answer = result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                with answer_placeholder.container():
                    render_answer_with_tables(final_answer)

                # ë””ë²„ê·¸ íŒ¨ë„
                if debug_mode:
                    with st.expander("ğŸ” ë””ë²„ê·¸ ì •ë³´", expanded=False):
                        col1, col2 = st.columns(2)

                        with col1:
                            st.subheader("Intent")
                            st.write(f"ë¶„ë¥˜: {result.get('intent', 'N/A')}")
                            st.write(f"ìƒˆ ì£¼ì œ: {result.get('is_new_topic', 'N/A')}")

                        with col2:
                            if result.get("plan"):
                                st.subheader("Plan")
                                st.json(result["plan"])

                        if result.get("retrieval"):
                            st.subheader("Retrieval")
                            st.write(f"ê²€ìƒ‰ íŒŒì¼: {result['retrieval'].get('files_searched', [])}")
                            st.write(f"ë¬¸ì„œ ìˆ˜: {len(result['retrieval'].get('docs', []))}")

                # ì„¸ì…˜ì— ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": final_answer})
                st.session_state.chat_history.append(HumanMessage(content=prompt))
                st.session_state.chat_history.append(AIMessage(content=final_answer))

                # chat_history ê¸¸ì´ ì œí•œ(ì›ë³¸ ìœ ì§€)
                if len(st.session_state.chat_history) > 20:
                    st.session_state.chat_history = st.session_state.chat_history[-20:]

            except Exception as e:
                status_placeholder.empty()
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                if debug_mode:
                    import traceback

                    st.code(traceback.format_exc())


# =========================================================
# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# =========================================================
if __name__ == "__main__":
    main()
